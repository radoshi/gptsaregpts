{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import baserun\n",
    "import openai\n",
    "import os\n",
    "import tiktoken\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple\n",
    "from tenacity import retry, wait_random_exponential\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rushabh/Library/Caches/pypoetry/virtualenvs/gptsaregpts-bRn197-3-py3.11/lib/python3.11/site-packages/baserun/baserun.py:44: UserWarning: Baserun has already been initialized. Additional calls to init will be ignored.\n",
      "  warnings.warn(\"Baserun has already been initialized. Additional calls to init will be ignored.\")\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "baserun.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "\n",
    "def connect(name: str) -> Tuple[sqlite3.Connection, sqlite3.Cursor]:\n",
    "    conn = sqlite3.connect(name)\n",
    "    c = conn.cursor()\n",
    "    return conn, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn, c = connect(\"../db.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the tokens for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"45\": 100, \"15\": 100, \"16\": 100, \"17\": 100, \"18\": 100}\n"
     ]
    }
   ],
   "source": [
    "def get_tokens() -> dict:\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    tokens = [encoding.encode(label) for label in [\"0\", \"1\", \"2\", \"3\", \"N\"]]\n",
    "\n",
    "    # Flatten the list of lists and convert to a set.\n",
    "    token_set = set([item for sublist in tokens for item in sublist])\n",
    "\n",
    "    # Create a dictionary with the tokens and a default value of 100.\n",
    "    token_dict = {str(token): 100 for token in token_set}\n",
    "\n",
    "    return token_dict\n",
    "\n",
    "\n",
    "logit_bias = get_tokens()\n",
    "print(json.dumps(logit_bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Create the classification table if it doesn't exist already.~~\n",
    "\n",
    "We don't need a classification table anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_table(c: sqlite3.Cursor, conn: sqlite3.Connection) -> None:\n",
    "    dwa_classification_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dwa_classification (\n",
    "        onetsoc_code CHARACTER(10) NOT NULL,\n",
    "        task_id DECIMAL(8,0) NOT NULL,    \n",
    "        dwa_id CHARACTER VARYING(20) NOT NULL,\n",
    "        classification CHARACTER(2) NOT NULL,\n",
    "        FOREIGN KEY (onetsoc_code) REFERENCES occupation_data(onetsoc_code),\n",
    "        FOREIGN KEY (task_id) REFERENCES task_statements(task_id),\n",
    "        FOREIGN KEY (dwa_id) REFERENCES dwa_reference(dwa_id),\n",
    "        PRIMARY KEY (onetsoc_code, task_id, dwa_id)\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    c.execute(dwa_classification_sql)\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "# create_classification_table(c, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "user_template = Path(Path.cwd().parent / \"prompts\" / \"user.txt\").read_text()\n",
    "system_template = Path(Path.cwd().parent / \"prompts\" / \"system.txt\").read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch all the rows from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWAReference(BaseModel):\n",
    "    dwa_id: str\n",
    "    dwa_title: str\n",
    "    classification: str | None = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_tuple(cls, tup: tuple):\n",
    "        return cls(\n",
    "            dwa_id=tup[0],\n",
    "            dwa_title=tup[1],\n",
    "        )\n",
    "    \n",
    "    @retry(wait=wait_random_exponential(multiplier=1, max=60))\n",
    "    async def classify(self) -> str:\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_template.format(\n",
    "                task=self.dwa_title,\n",
    "            ),\n",
    "        }\n",
    "        system_message = {\"role\": \"system\", \"content\": system_template}\n",
    "\n",
    "        response = await openai.ChatCompletion.acreate(\n",
    "            messages=[system_message, user_message],\n",
    "            logit_bias=logit_bias,\n",
    "            max_tokens=1,\n",
    "            temperature=0,\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "        )\n",
    "        self.classification = response.choices[0][\"message\"][\"content\"]\n",
    "        return self.classification\n",
    "\n",
    "    def save(self, c: sqlite3.Cursor) -> None:\n",
    "        insert_sql = \"UPDATE dwa_reference SET classification = ? WHERE dwa_id = ?\"\n",
    "        c.execute(insert_sql, (self.classification, self.dwa_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_rows(c: sqlite3.Cursor, num: int = -1) -> list:\n",
    "    select_query = \"\"\"\n",
    "select    \n",
    "  t.dwa_id,\n",
    "  t.dwa_title\n",
    "from\n",
    "  dwa_reference as t\n",
    "where\n",
    "  t.classification is null\n",
    "order by\n",
    "  t.dwa_id\n",
    "  \"\"\"\n",
    "    c.execute(select_query)\n",
    "    if num == -1:\n",
    "        return [DWAReference.from_tuple(r) for r in c.fetchall()]\n",
    "    else:\n",
    "        return [DWAReference.from_tuple(r) for r in c.fetchmany(num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def classify_all(rows: list[DWAReference]) -> None:\n",
    "    for row in tqdm(rows, desc=\"Classifying\"):\n",
    "        await row.classify()\n",
    "        print(f\"[{row.dwa_id}]: {row.dwa_title}: {row.classification}\")\n",
    "        row.save(c)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7d32eaf0c4424582352583e435d7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = fetch_rows(c, 1000)\n",
    "await classify_all(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptsaregpts-bRn197-3-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
